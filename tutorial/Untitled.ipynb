{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ce0f122-623f-4b45-9984-47791ef56b92",
   "metadata": {},
   "source": [
    "## 时序的关于时间的embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc2e11c6-208f-4e1f-8c38-e1174eb6430a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7dec729b-19de-424c-8188-f9cff045ae12",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FixedEmbedding(nn.Module):\n",
    "    def __init__(self, c_in, d_model):\n",
    "        super(FixedEmbedding, self).__init__()\n",
    "\n",
    "        w = torch.zeros(c_in, d_model).float()\n",
    "        w.require_grad = False\n",
    "\n",
    "        position = torch.arange(0, c_in).float().unsqueeze(1)\n",
    "        div_term = (torch.arange(0, d_model, 2).float() *\n",
    "                    -(math.log(10000.0) / d_model)).exp()\n",
    "\n",
    "        w[:, 0::2] = torch.sin(position * div_term)\n",
    "        w[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        self.emb = nn.Embedding(c_in, d_model)\n",
    "        self.emb.weight = nn.Parameter(w, requires_grad=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.emb(x).detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aed79dae-e021-4196-981f-781c875459f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TemporalEmbedding(nn.Module):\n",
    "    def __init__(self, d_model, embed_type='fixed', freq='h'):\n",
    "        super(TemporalEmbedding, self).__init__()\n",
    "\n",
    "        minute_size = 4\n",
    "        hour_size = 24\n",
    "        weekday_size = 7\n",
    "        day_size = 32\n",
    "        month_size = 13\n",
    "\n",
    "        Embed = FixedEmbedding if embed_type == 'fixed' else nn.Embedding\n",
    "        if freq == 't':\n",
    "            self.minute_embed = Embed(minute_size, d_model)\n",
    "        self.hour_embed = Embed(hour_size, d_model)\n",
    "        self.weekday_embed = Embed(weekday_size, d_model)\n",
    "        self.day_embed = Embed(day_size, d_model)\n",
    "        self.month_embed = Embed(month_size, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.long()\n",
    "        minute_x = self.minute_embed(x[:, :, 4]) if hasattr(\n",
    "            self, 'minute_embed') else 0.\n",
    "        hour_x = self.hour_embed(x[:, :, 3])\n",
    "        weekday_x = self.weekday_embed(x[:, :, 2])\n",
    "        day_x = self.day_embed(x[:, :, 1])\n",
    "        month_x = self.month_embed(x[:, :, 0])\n",
    "\n",
    "        return hour_x + weekday_x + day_x + month_x + minute_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f69f18bc-955d-4c46-8830-7035e3406cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeFeatureEmbedding(nn.Module):\n",
    "    def __init__(self, d_model, embed_type='timeF', freq='h'):\n",
    "        super(TimeFeatureEmbedding, self).__init__()\n",
    "\n",
    "        freq_map = {'h': 4, 't': 5, 's': 6,\n",
    "                    'm': 1, 'a': 1, 'w': 2, 'd': 3, 'b': 3}\n",
    "        d_inp = freq_map[freq]\n",
    "        self.embed = nn.Linear(d_inp, d_model, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        import ipdb; ipdb.set_trace()\n",
    "        return self.embed(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d9bbe2e8-e100-4d83-91ca-a23e43e0b0f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建时间序列数据\n",
    "time_data = torch.arange(0, 24*7)  # 一周的小时数\n",
    "time_data = time_data.unsqueeze(1)  # 添加 batch 维度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f2d5925e-3044-491f-85c7-91d203391639",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The expanded size of the tensor (1) must match the existing size (2) at non-singleton dimension 1.  Target sizes: [168, 1].  Tensor sizes: [168, 2]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# 创建固定嵌入层和时间嵌入层\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m fixed_emb \u001b[38;5;241m=\u001b[39m \u001b[43mFixedEmbedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mc_in\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m24\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m7\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43md_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m temporal_emb \u001b[38;5;241m=\u001b[39m TemporalEmbedding(d_model\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, embed_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfixed\u001b[39m\u001b[38;5;124m'\u001b[39m, freq\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mh\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[10], line 13\u001b[0m, in \u001b[0;36mFixedEmbedding.__init__\u001b[0;34m(self, c_in, d_model)\u001b[0m\n\u001b[1;32m      9\u001b[0m div_term \u001b[38;5;241m=\u001b[39m (torch\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;241m0\u001b[39m, d_model, \u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mfloat() \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m     10\u001b[0m             \u001b[38;5;241m-\u001b[39m(math\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;241m10000.0\u001b[39m) \u001b[38;5;241m/\u001b[39m d_model))\u001b[38;5;241m.\u001b[39mexp()\n\u001b[1;32m     12\u001b[0m w[:, \u001b[38;5;241m0\u001b[39m::\u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msin(position \u001b[38;5;241m*\u001b[39m div_term)\n\u001b[0;32m---> 13\u001b[0m \u001b[43mw\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcos(position \u001b[38;5;241m*\u001b[39m div_term)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39memb \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mEmbedding(c_in, d_model)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39memb\u001b[38;5;241m.\u001b[39mweight \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mParameter(w, requires_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The expanded size of the tensor (1) must match the existing size (2) at non-singleton dimension 1.  Target sizes: [168, 1].  Tensor sizes: [168, 2]"
     ]
    }
   ],
   "source": [
    "# 创建固定嵌入层和时间嵌入层\n",
    "fixed_emb = FixedEmbedding(c_in=24*7, d_model=3)\n",
    "temporal_emb = TemporalEmbedding(d_model=3, embed_type='fixed', freq='h')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ba03a5-eddc-48bb-ae63-64468b08a756",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算固定嵌入和时间嵌入\n",
    "fixed_embedding = fixed_emb(time_data)\n",
    "temporal_embedding = temporal_emb(time_data)\n",
    "\n",
    "# 将嵌入向量转换为 numpy 数组\n",
    "fixed_embedding_np = fixed_embedding.detach().numpy()\n",
    "temporal_embedding_np = temporal_embedding.detach().numpy()\n",
    "\n",
    "# 绘制固定嵌入向量的三维散点图\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(121, projection='3d')\n",
    "ax.scatter(fixed_embedding_np[:, 0], fixed_embedding_np[:, 1], fixed_embedding_np[:, 2], c='r', marker='o')\n",
    "ax.set_title('Fixed Embedding')\n",
    "ax.set_xlabel('X')\n",
    "ax.set_ylabel('Y')\n",
    "ax.set_zlabel('Z')\n",
    "\n",
    "# 绘制时间嵌入向量的三维散点图\n",
    "ax = fig.add_subplot(122, projection='3d')\n",
    "ax.scatter(temporal_embedding_np[:, 0], temporal_embedding_np[:, 1], temporal_embedding_np[:, 2], c='b', marker='o')\n",
    "ax.set_title('Temporal Embedding')\n",
    "ax.set_xlabel('X')\n",
    "ax.set_ylabel('Y')\n",
    "ax.set_zlabel('Z')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "#这个演示程序会生成两个三维散点图，一个是固定嵌入层生成的嵌入向量，另一个是时间嵌入层生成的嵌入向量。你可以观察到，固定嵌入层生成的向量是围绕着某个中心点分布的，而时间嵌入层生成的向量则显示出明显的周期性和规律性，这反映了时间信息的编码效果。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "951fd89a-7970-495e-b2d3-e493f3f67ed4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
